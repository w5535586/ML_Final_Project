{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1b8bec",
   "metadata": {
    "papermill": {
     "duration": 0.009655,
     "end_time": "2023-12-19T14:00:29.369567",
     "exception": false,
     "start_time": "2023-12-19T14:00:29.359912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a fork of [best-public-score](https://www.kaggle.com/code/peizhengwang/best-public-score) notebook with some tuned hyperparameters for LightGBM model, that result in a slight improvement of LB score: 5.3365 -> 5.3359. All the credits go to the original author @peizhengwang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be65df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:29.388651Z",
     "iopub.status.busy": "2023-12-19T14:00:29.388249Z",
     "iopub.status.idle": "2023-12-19T14:00:34.991263Z",
     "shell.execute_reply": "2023-12-19T14:00:34.990441Z"
    },
    "papermill": {
     "duration": 5.61523,
     "end_time": "2023-12-19T14:00:34.993648",
     "exception": false,
     "start_time": "2023-12-19T14:00:29.378418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "import polars as pl\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "is_offline = False \n",
    "LGB = False\n",
    "CAT_train=False\n",
    "CAT=True\n",
    "NN = False\n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0260832",
   "metadata": {
    "papermill": {
     "duration": 0.007614,
     "end_time": "2023-12-19T14:00:35.009535",
     "exception": false,
     "start_time": "2023-12-19T14:00:35.001921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da019c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:35.026635Z",
     "iopub.status.busy": "2023-12-19T14:00:35.025866Z",
     "iopub.status.idle": "2023-12-19T14:00:35.031370Z",
     "shell.execute_reply": "2023-12-19T14:00:35.030511Z"
    },
    "papermill": {
     "duration": 0.016247,
     "end_time": "2023-12-19T14:00:35.033393",
     "exception": false,
     "start_time": "2023-12-19T14:00:35.017146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_average(a):\n",
    "    w = []\n",
    "    n = len(a)\n",
    "    for j in range(1, n + 1):\n",
    "        j = 2 if j == 1 else j\n",
    "        w.append(1 / (2**(n + 1 - j)))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e40056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:35.052718Z",
     "iopub.status.busy": "2023-12-19T14:00:35.052459Z",
     "iopub.status.idle": "2023-12-19T14:00:35.065814Z",
     "shell.execute_reply": "2023-12-19T14:00:35.064988Z"
    },
    "papermill": {
     "duration": 0.02519,
     "end_time": "2023-12-19T14:00:35.067668",
     "exception": false,
     "start_time": "2023-12-19T14:00:35.042478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=0):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f77120",
   "metadata": {
    "papermill": {
     "duration": 0.007472,
     "end_time": "2023-12-19T14:00:35.082779",
     "exception": false,
     "start_time": "2023-12-19T14:00:35.075307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ae2298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:35.099540Z",
     "iopub.status.busy": "2023-12-19T14:00:35.099024Z",
     "iopub.status.idle": "2023-12-19T14:00:53.583431Z",
     "shell.execute_reply": "2023-12-19T14:00:53.582597Z"
    },
    "papermill": {
     "duration": 18.495277,
     "end_time": "2023-12-19T14:00:53.585729",
     "exception": false,
     "start_time": "2023-12-19T14:00:35.090452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "df = df.dropna(subset=[\"target\"])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_shape = df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b8b8b",
   "metadata": {
    "papermill": {
     "duration": 0.007824,
     "end_time": "2023-12-19T14:00:53.601716",
     "exception": false,
     "start_time": "2023-12-19T14:00:53.593892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Outlier removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a71ab",
   "metadata": {
    "papermill": {
     "duration": 0.007646,
     "end_time": "2023-12-19T14:00:53.617250",
     "exception": false,
     "start_time": "2023-12-19T14:00:53.609604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Parallel Triplet Imbalance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69185716",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:53.635462Z",
     "iopub.status.busy": "2023-12-19T14:00:53.635161Z",
     "iopub.status.idle": "2023-12-19T14:00:54.338742Z",
     "shell.execute_reply": "2023-12-19T14:00:54.337842Z"
    },
    "papermill": {
     "duration": 0.71612,
     "end_time": "2023-12-19T14:00:54.341306",
     "exception": false,
     "start_time": "2023-12-19T14:00:53.625186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82bd30",
   "metadata": {
    "papermill": {
     "duration": 0.00774,
     "end_time": "2023-12-19T14:00:54.357339",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.349599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Generation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50fada8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:54.375203Z",
     "iopub.status.busy": "2023-12-19T14:00:54.374619Z",
     "iopub.status.idle": "2023-12-19T14:00:54.402630Z",
     "shell.execute_reply": "2023-12-19T14:00:54.401684Z"
    },
    "papermill": {
     "duration": 0.039282,
     "end_time": "2023-12-19T14:00:54.404764",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.365482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    \n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,2,3,4,5,6,7,8,9,10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,2,3,4,5,6,7,8,9,10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    #V4 feature\n",
    "    for window in [2,3,4,5,6,7,8,9,10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    #V5 - rolling diff\n",
    "    # Convert from pandas to Polars\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    #Define the windows and columns for which you want to calculate the rolling statistics\n",
    "    windows = [2,3,4,5,6,7,8,9,10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    # prepare the operations for each column and window\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    # Loop over each window and column to create the rolling mean and std expressions\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    # Run the operations using Polars' lazy API\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    # Execute the lazy expressions and overwrite the pl_df variable\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    # Convert back to pandas if necessary\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_all_features(df):\n",
    "    # Select relevant columns for feature generation\n",
    "    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    gc.collect() \n",
    "    df = other_features(df)\n",
    "    gc.collect()  \n",
    "    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n",
    "    \n",
    "    return df[feature_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fc2523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:54.422588Z",
     "iopub.status.busy": "2023-12-19T14:00:54.422278Z",
     "iopub.status.idle": "2023-12-19T14:00:54.435329Z",
     "shell.execute_reply": "2023-12-19T14:00:54.434356Z"
    },
    "papermill": {
     "duration": 0.024442,
     "end_time": "2023-12-19T14:00:54.437360",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.412918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba0a0a",
   "metadata": {
    "papermill": {
     "duration": 0.008588,
     "end_time": "2023-12-19T14:00:54.454369",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.445781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6a7348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:54.472674Z",
     "iopub.status.busy": "2023-12-19T14:00:54.472314Z",
     "iopub.status.idle": "2023-12-19T14:00:54.478249Z",
     "shell.execute_reply": "2023-12-19T14:00:54.477334Z"
    },
    "papermill": {
     "duration": 0.017881,
     "end_time": "2023-12-19T14:00:54.480529",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.462648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online mode\n"
     ]
    }
   ],
   "source": [
    "if is_offline:\n",
    "    \n",
    "    df_train = df[df[\"date_id\"] <= split_day]\n",
    "    df_valid = df[df[\"date_id\"] > split_day]\n",
    "    print(\"Offline mode\")\n",
    "    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\n",
    "    \n",
    "else:\n",
    "    df_train = df\n",
    "    print(\"Online mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22655cc",
   "metadata": {
    "papermill": {
     "duration": 0.008327,
     "end_time": "2023-12-19T14:00:54.497634",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.489307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e933ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:54.516768Z",
     "iopub.status.busy": "2023-12-19T14:00:54.516406Z",
     "iopub.status.idle": "2023-12-19T14:00:56.152785Z",
     "shell.execute_reply": "2023-12-19T14:00:56.151958Z"
    },
    "papermill": {
     "duration": 1.649026,
     "end_time": "2023-12-19T14:00:56.155169",
     "exception": false,
     "start_time": "2023-12-19T14:00:54.506143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    feature_columns=['stock_id',\n",
    " 'seconds_in_bucket',\n",
    " 'imbalance_size',\n",
    " 'imbalance_buy_sell_flag',\n",
    " 'reference_price',\n",
    " 'matched_size',\n",
    " 'far_price',\n",
    " 'near_price',\n",
    " 'bid_price',\n",
    " 'bid_size',\n",
    " 'ask_price',\n",
    " 'ask_size',\n",
    " 'wap',\n",
    " 'volume',\n",
    " 'mid_price',\n",
    " 'liquidity_imbalance',\n",
    " 'matched_imbalance',\n",
    " 'size_imbalance',\n",
    " 'reference_price_far_price_imb',\n",
    " 'reference_price_near_price_imb',\n",
    " 'reference_price_ask_price_imb',\n",
    " 'reference_price_bid_price_imb',\n",
    " 'reference_price_wap_imb',\n",
    " 'far_price_near_price_imb',\n",
    " 'far_price_ask_price_imb',\n",
    " 'far_price_bid_price_imb',\n",
    " 'far_price_wap_imb',\n",
    " 'near_price_ask_price_imb',\n",
    " 'near_price_bid_price_imb',\n",
    " 'near_price_wap_imb',\n",
    " 'ask_price_bid_price_imb',\n",
    " 'ask_price_wap_imb',\n",
    " 'bid_price_wap_imb',\n",
    " 'ask_price_bid_price_wap_imb2',\n",
    " 'ask_price_bid_price_reference_price_imb2',\n",
    " 'ask_price_wap_reference_price_imb2',\n",
    " 'bid_price_wap_reference_price_imb2',\n",
    " 'matched_size_bid_size_ask_size_imb2',\n",
    " 'matched_size_bid_size_imbalance_size_imb2',\n",
    " 'matched_size_ask_size_imbalance_size_imb2',\n",
    " 'bid_size_ask_size_imbalance_size_imb2',\n",
    " 'stock_weights',\n",
    " 'weighted_wap',\n",
    " 'wap_momentum',\n",
    " 'imbalance_momentum',\n",
    " 'price_spread',\n",
    " 'spread_intensity',\n",
    " 'price_pressure',\n",
    " 'market_urgency',\n",
    " 'depth_pressure',\n",
    " 'spread_depth_ratio',\n",
    " 'mid_price_movement',\n",
    " 'micro_price',\n",
    " 'relative_spread',\n",
    " 'all_prices_mean',\n",
    " 'all_sizes_mean',\n",
    " 'all_prices_std',\n",
    " 'all_sizes_std',\n",
    " 'all_prices_skew',\n",
    " 'all_sizes_skew',\n",
    " 'all_prices_kurt',\n",
    " 'all_sizes_kurt',\n",
    " 'matched_size_shift_1',\n",
    " 'matched_size_ret_1',\n",
    " 'matched_size_shift_3',\n",
    " 'matched_size_ret_3',\n",
    " 'matched_size_shift_5',\n",
    " 'matched_size_ret_5',\n",
    " 'matched_size_shift_10',\n",
    " 'matched_size_ret_10',\n",
    " 'imbalance_size_shift_1',\n",
    " 'imbalance_size_ret_1',\n",
    " 'imbalance_size_shift_3',\n",
    " 'imbalance_size_ret_3',\n",
    " 'imbalance_size_shift_5',\n",
    " 'imbalance_size_ret_5',\n",
    " 'imbalance_size_shift_10',\n",
    " 'imbalance_size_ret_10',\n",
    " 'reference_price_shift_1',\n",
    " 'reference_price_ret_1',\n",
    " 'reference_price_shift_3',\n",
    " 'reference_price_ret_3',\n",
    " 'reference_price_shift_5',\n",
    " 'reference_price_ret_5',\n",
    " 'reference_price_shift_10',\n",
    " 'reference_price_ret_10',\n",
    " 'imbalance_buy_sell_flag_shift_1',\n",
    " 'imbalance_buy_sell_flag_ret_1',\n",
    " 'imbalance_buy_sell_flag_shift_3',\n",
    " 'imbalance_buy_sell_flag_ret_3',\n",
    " 'imbalance_buy_sell_flag_shift_5',\n",
    " 'imbalance_buy_sell_flag_ret_5',\n",
    " 'imbalance_buy_sell_flag_shift_10',\n",
    " 'imbalance_buy_sell_flag_ret_10',\n",
    " 'ask_price_diff_1',\n",
    " 'ask_price_diff_3',\n",
    " 'ask_price_diff_5',\n",
    " 'ask_price_diff_10',\n",
    " 'bid_price_diff_1',\n",
    " 'bid_price_diff_3',\n",
    " 'bid_price_diff_5',\n",
    " 'bid_price_diff_10',\n",
    " 'ask_size_diff_1',\n",
    " 'ask_size_diff_3',\n",
    " 'ask_size_diff_5',\n",
    " 'ask_size_diff_10',\n",
    " 'bid_size_diff_1',\n",
    " 'bid_size_diff_3',\n",
    " 'bid_size_diff_5',\n",
    " 'bid_size_diff_10',\n",
    " 'weighted_wap_diff_1',\n",
    " 'weighted_wap_diff_3',\n",
    " 'weighted_wap_diff_5',\n",
    " 'weighted_wap_diff_10',\n",
    " 'price_spread_diff_1',\n",
    " 'price_spread_diff_3',\n",
    " 'price_spread_diff_5',\n",
    " 'price_spread_diff_10',\n",
    " 'price_change_diff_3',\n",
    " 'size_change_diff_3',\n",
    " 'price_change_diff_5',\n",
    " 'size_change_diff_5',\n",
    " 'price_change_diff_10',\n",
    " 'size_change_diff_10',\n",
    " 'rolling_diff_ask_price_3',\n",
    " 'rolling_std_diff_ask_price_3',\n",
    " 'rolling_diff_bid_price_3',\n",
    " 'rolling_std_diff_bid_price_3',\n",
    " 'rolling_diff_ask_size_3',\n",
    " 'rolling_std_diff_ask_size_3',\n",
    " 'rolling_diff_bid_size_3',\n",
    " 'rolling_std_diff_bid_size_3',\n",
    " 'rolling_diff_ask_price_5',\n",
    " 'rolling_std_diff_ask_price_5',\n",
    " 'rolling_diff_bid_price_5',\n",
    " 'rolling_std_diff_bid_price_5',\n",
    " 'rolling_diff_ask_size_5',\n",
    " 'rolling_std_diff_ask_size_5',\n",
    " 'rolling_diff_bid_size_5',\n",
    " 'rolling_std_diff_bid_size_5',\n",
    " 'rolling_diff_ask_price_10',\n",
    " 'rolling_std_diff_ask_price_10',\n",
    " 'rolling_diff_bid_price_10',\n",
    " 'rolling_std_diff_bid_price_10',\n",
    " 'rolling_diff_ask_size_10',\n",
    " 'rolling_std_diff_ask_size_10',\n",
    " 'rolling_diff_bid_size_10',\n",
    " 'rolling_std_diff_bid_size_10',\n",
    " 'mid_price*volume',\n",
    " 'harmonic_imbalance',\n",
    " 'dow',\n",
    " 'seconds',\n",
    " 'minute',\n",
    " 'time_to_market_close',\n",
    " 'global_median_size',\n",
    " 'global_std_size',\n",
    " 'global_ptp_size',\n",
    " 'global_median_price',\n",
    " 'global_std_price',\n",
    " 'global_ptp_price']\n",
    "    global_stock_id_feats = {\n",
    "        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "    }\n",
    "#     if is_offline:\n",
    "#         df_train_feats = generate_all_features(df_train)\n",
    "#         print(\"Build Train Feats Finished.\")\n",
    "#         df_valid_feats = generate_all_features(df_valid)\n",
    "#         print(\"Build Valid Feats Finished.\")\n",
    "#         df_valid_feats = reduce_mem_usage(df_valid_feats)\n",
    "#     else:\n",
    "#         df_train_feats = generate_all_features(df_train)\n",
    "#         print(\"Build Online Train Feats Finished.\")\n",
    "\n",
    "#     df_train_feats = reduce_mem_usage(df_train_feats)\n",
    "    del df_train\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1396c74",
   "metadata": {
    "papermill": {
     "duration": 0.007752,
     "end_time": "2023-12-19T14:00:56.171335",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.163583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123f9f4",
   "metadata": {
    "papermill": {
     "duration": 0.008087,
     "end_time": "2023-12-19T14:00:56.187339",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.179252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **CAT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1802490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:56.205233Z",
     "iopub.status.busy": "2023-12-19T14:00:56.204891Z",
     "iopub.status.idle": "2023-12-19T14:00:56.609851Z",
     "shell.execute_reply": "2023-12-19T14:00:56.609012Z"
    },
    "papermill": {
     "duration": 0.416911,
     "end_time": "2023-12-19T14:00:56.612273",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.195362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\n",
    "if CAT_train:\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    cat_params = dict(iterations=7000,\n",
    "                      learning_rate=0.01,\n",
    "                      depth=11,\n",
    "                      l2_leaf_reg=30,\n",
    "                      bootstrap_type='Bernoulli',\n",
    "                      subsample=0.66,\n",
    "                      loss_function='MAE',\n",
    "                      eval_metric = 'MAE',\n",
    "                      metric_period=100,\n",
    "                      od_type='Iter',\n",
    "                      od_wait=30,\n",
    "                      task_type='GPU',\n",
    "                      allow_writing_files=False,\n",
    "                      )\n",
    "\n",
    "    feature_columns = list(df_train_feats.columns)\n",
    "    print(f\"Features = {len(feature_columns)}\")\n",
    "    #print(f\"Feature length = {len(feature_columns)}\")\n",
    "\n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "\n",
    "    models = []\n",
    "    models_cbt = []\n",
    "    scores = []\n",
    "\n",
    "    model_save_path = 'modelitos_para_despues_cat' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    date_ids = df_train['date_id'].values\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "\n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "\n",
    "        # Train a LightGBM model for the current fold\n",
    "        cat_model = CatBoostRegressor(**cat_params)\n",
    "        cat_model.fit(\n",
    "            df_fold_train[feature_columns],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "        models.append(cat_model)\n",
    "        \n",
    "        \n",
    "        # Save the model to a file\n",
    "        #model_filename = os.path.join(model_save_path, f'doblez_{i+1}.model')\n",
    "        #joblib.dump(cat_model, model_filename)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "        #lgb_model.booster_.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "        # Evaluate model performance on the validation set\n",
    "        #------------LGB--------------#\n",
    "        fold_predictions = cat_model.predict(df_fold_valid[feature_columns])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\":LGB Fold {i+1} MAE: {fold_score}\")\n",
    "        #------------CBT--------------#\n",
    "#         fold_predictions = cbt_model.predict(df_fold_valid[feature_columns])\n",
    "#         fold_score_cbt = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "#         scores.append(fold_score_cbt)\n",
    "#         print(f\"CBT Fold {i+1} MAE: {fold_score_cbt}\")\n",
    "\n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "\n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = cat_params.copy()\n",
    "\n",
    "    # final_model_params['n_estimators'] = average_best_iteration\n",
    "    # print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "    # Train the final model on the entire dataset\n",
    "    num_model = 1\n",
    "\n",
    "    for i in range(num_model):\n",
    "        final_model = CatBoostRegressor(**final_model_params)\n",
    "        final_model.fit(\n",
    "            df_train_feats[feature_columns],\n",
    "            df_train['target'],\n",
    "\n",
    "        )\n",
    "        #model_filename = os.path.join(model_save_path, f'total_{i+1}.model')\n",
    "        #joblib.dump(cat_model, model_filename)\n",
    "        \n",
    "        # Append the final model to the list of models\n",
    "        models.append(final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a462d0d",
   "metadata": {
    "papermill": {
     "duration": 0.007935,
     "end_time": "2023-12-19T14:00:56.628728",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.620793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8607f649",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:56.647462Z",
     "iopub.status.busy": "2023-12-19T14:00:56.646721Z",
     "iopub.status.idle": "2023-12-19T14:00:56.664635Z",
     "shell.execute_reply": "2023-12-19T14:00:56.663793Z"
    },
    "papermill": {
     "duration": 0.029474,
     "end_time": "2023-12-19T14:00:56.666571",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.637097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if LGB:\n",
    "    import numpy as np\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 6000,\n",
    "        \"num_leaves\": 256,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "#         \"learning_rate\": 0.00871,\n",
    "        \"learning_rate\": 0.01,\n",
    "        'max_depth': 11,\n",
    "        \"n_jobs\": 4,\n",
    "        \"device\": \"gpu\",\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "#         \"reg_alpha\": 0.1,\n",
    "        \"reg_alpha\": 0.2,\n",
    "        \"reg_lambda\": 3.25\n",
    "    }\n",
    "\n",
    "    feature_columns = list(df_train_feats.columns)\n",
    "    print(f\"Features = {len(feature_columns)}\")\n",
    "    #print(f\"Feature length = {len(feature_columns)}\")\n",
    "\n",
    "    num_folds = 5\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "\n",
    "    models = []\n",
    "    models_cbt = []\n",
    "    scores = []\n",
    "\n",
    "    model_save_path = 'modelitos_para_despues' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    date_ids = df_train['date_id'].values\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "\n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "        df_fold_train = df_train_feats[train_indices]\n",
    "        df_fold_train_target = df_train['target'][train_indices]\n",
    "        df_fold_valid = df_train_feats[test_indices]\n",
    "        df_fold_valid_target = df_train['target'][test_indices]\n",
    "\n",
    "        print(f\"Fold {i+1} Model Training\")\n",
    "\n",
    "        # Train a LightGBM model for the current fold\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(\n",
    "            df_fold_train[feature_columns],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "#         cbt_model = cbt.CatBoostRegressor(objective='MAE', iterations=5000,bagging_temperature=0.5,\n",
    "#                                 colsample_bylevel = 0.7,learning_rate = 0.065,\n",
    "#                                 od_wait = 25,max_depth = 7,l2_leaf_reg = 1.5,\n",
    "#                                 min_data_in_leaf = 1000,random_strength=0.65,\n",
    "#                                 verbose=0,use_best_model=True,task_type='CPU')\n",
    "#         cbt_model.fit(\n",
    "#             df_fold_train[feature_columns],\n",
    "#             df_fold_train_target,\n",
    "#             eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)]\n",
    "#         )\n",
    "        \n",
    "#         models_cbt.append(cbt_model)\n",
    "\n",
    "        models.append(lgb_model)\n",
    "        # Save the model to a file\n",
    "        model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n",
    "        lgb_model.booster_.save_model(model_filename)\n",
    "        print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "\n",
    "        # Evaluate model performance on the validation set\n",
    "        #------------LGB--------------#\n",
    "        fold_predictions = lgb_model.predict(df_fold_valid[feature_columns])\n",
    "        fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f\":LGB Fold {i+1} MAE: {fold_score}\")\n",
    "        #------------CBT--------------#\n",
    "#         fold_predictions = cbt_model.predict(df_fold_valid[feature_columns])\n",
    "#         fold_score_cbt = mean_absolute_error(fold_predictions, df_fold_valid_target)\n",
    "#         scores.append(fold_score_cbt)\n",
    "#         print(f\"CBT Fold {i+1} MAE: {fold_score_cbt}\")\n",
    "\n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "\n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = lgb_params.copy()\n",
    "\n",
    "    # final_model_params['n_estimators'] = average_best_iteration\n",
    "    # print(f\"Training final model with average best iteration: {average_best_iteration}\")\n",
    "\n",
    "    # Train the final model on the entire dataset\n",
    "    num_model = 1\n",
    "\n",
    "    for i in range(num_model):\n",
    "        final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "        final_model.fit(\n",
    "            df_train_feats[feature_columns],\n",
    "            df_train['target'],\n",
    "            callbacks=[\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "        # Append the final model to the list of models\n",
    "        models.append(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f7d29f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:56.684389Z",
     "iopub.status.busy": "2023-12-19T14:00:56.684091Z",
     "iopub.status.idle": "2023-12-19T14:00:56.702790Z",
     "shell.execute_reply": "2023-12-19T14:00:56.701925Z"
    },
    "papermill": {
     "duration": 0.030162,
     "end_time": "2023-12-19T14:00:56.704752",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.674590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_feature=['seconds_in_bucket',\n",
    " 'imbalance_size',\n",
    " 'imbalance_buy_sell_flag',\n",
    " 'reference_price',\n",
    " 'matched_size',\n",
    " 'far_price',\n",
    " 'bid_price',\n",
    " 'bid_size',\n",
    " 'ask_price',\n",
    " 'ask_size',\n",
    " 'wap',\n",
    " 'volume',\n",
    " 'mid_price',\n",
    " 'liquidity_imbalance',\n",
    " 'matched_imbalance',\n",
    " 'size_imbalance',\n",
    " 'reference_price_ask_price_imb',\n",
    " 'reference_price_bid_price_imb',\n",
    " 'reference_price_wap_imb',\n",
    " 'far_price_near_price_imb',\n",
    " 'far_price_wap_imb',\n",
    " 'near_price_wap_imb',\n",
    " 'ask_price_bid_price_imb',\n",
    " 'bid_price_wap_imb',\n",
    " 'ask_price_bid_price_wap_imb2',\n",
    " 'ask_price_bid_price_reference_price_imb2',\n",
    " 'ask_price_wap_reference_price_imb2',\n",
    " 'bid_price_wap_reference_price_imb2',\n",
    " 'matched_size_bid_size_ask_size_imb2',\n",
    " 'matched_size_bid_size_imbalance_size_imb2',\n",
    " 'matched_size_ask_size_imbalance_size_imb2',\n",
    " 'bid_size_ask_size_imbalance_size_imb2',\n",
    " 'wap_momentum',\n",
    " 'imbalance_momentum',\n",
    " 'price_spread',\n",
    " 'spread_intensity',\n",
    " 'price_pressure',\n",
    " 'market_urgency',\n",
    " 'depth_pressure',\n",
    " 'spread_depth_ratio',\n",
    " 'mid_price_movement',\n",
    " 'micro_price',\n",
    " 'all_sizes_std',\n",
    " 'all_sizes_skew',\n",
    " 'all_sizes_kurt',\n",
    " 'matched_size_shift_1',\n",
    " 'matched_size_ret_1',\n",
    " 'matched_size_shift_2',\n",
    " 'matched_size_ret_2',\n",
    " 'matched_size_shift_3',\n",
    " 'matched_size_ret_3',\n",
    " 'matched_size_shift_4',\n",
    " 'matched_size_shift_5',\n",
    " 'matched_size_shift_6',\n",
    " 'matched_size_shift_7',\n",
    " 'matched_size_shift_8',\n",
    " 'matched_size_shift_9',\n",
    " 'matched_size_shift_10',\n",
    " 'imbalance_size_shift_1',\n",
    " 'imbalance_size_ret_1',\n",
    " 'imbalance_size_shift_2',\n",
    " 'imbalance_size_ret_2',\n",
    " 'imbalance_size_shift_3',\n",
    " 'imbalance_size_ret_3',\n",
    " 'imbalance_size_shift_4',\n",
    " 'imbalance_size_ret_4',\n",
    " 'imbalance_size_shift_5',\n",
    " 'imbalance_size_ret_5',\n",
    " 'imbalance_size_shift_6',\n",
    " 'imbalance_size_ret_6',\n",
    " 'imbalance_size_shift_7',\n",
    " 'imbalance_size_ret_7',\n",
    " 'imbalance_size_shift_8',\n",
    " 'imbalance_size_ret_8',\n",
    " 'imbalance_size_shift_9',\n",
    " 'imbalance_size_ret_9',\n",
    " 'imbalance_size_shift_10',\n",
    " 'imbalance_size_ret_10',\n",
    " 'reference_price_shift_1',\n",
    " 'reference_price_ret_1',\n",
    " 'reference_price_shift_2',\n",
    " 'reference_price_ret_2',\n",
    " 'reference_price_shift_3',\n",
    " 'reference_price_ret_3',\n",
    " 'reference_price_shift_4',\n",
    " 'reference_price_ret_4',\n",
    " 'reference_price_shift_5',\n",
    " 'reference_price_ret_5',\n",
    " 'reference_price_shift_6',\n",
    " 'reference_price_ret_6',\n",
    " 'reference_price_shift_7',\n",
    " 'reference_price_ret_7',\n",
    " 'reference_price_shift_8',\n",
    " 'reference_price_ret_8',\n",
    " 'reference_price_shift_9',\n",
    " 'reference_price_ret_9',\n",
    " 'reference_price_shift_10',\n",
    " 'reference_price_ret_10',\n",
    " 'imbalance_buy_sell_flag_shift_1',\n",
    " 'imbalance_buy_sell_flag_ret_1',\n",
    " 'imbalance_buy_sell_flag_shift_2',\n",
    " 'imbalance_buy_sell_flag_ret_2',\n",
    " 'imbalance_buy_sell_flag_shift_3',\n",
    " 'imbalance_buy_sell_flag_ret_3',\n",
    " 'imbalance_buy_sell_flag_shift_4',\n",
    " 'imbalance_buy_sell_flag_ret_4',\n",
    " 'imbalance_buy_sell_flag_shift_5',\n",
    " 'imbalance_buy_sell_flag_ret_5',\n",
    " 'imbalance_buy_sell_flag_shift_6',\n",
    " 'imbalance_buy_sell_flag_ret_6',\n",
    " 'imbalance_buy_sell_flag_shift_7',\n",
    " 'imbalance_buy_sell_flag_ret_7',\n",
    " 'imbalance_buy_sell_flag_shift_8',\n",
    " 'imbalance_buy_sell_flag_ret_8',\n",
    " 'imbalance_buy_sell_flag_shift_9',\n",
    " 'imbalance_buy_sell_flag_ret_9',\n",
    " 'imbalance_buy_sell_flag_shift_10',\n",
    " 'imbalance_buy_sell_flag_ret_10',\n",
    " 'ask_price_diff_1',\n",
    " 'ask_price_diff_2',\n",
    " 'ask_price_diff_3',\n",
    " 'ask_price_diff_4',\n",
    " 'ask_price_diff_5',\n",
    " 'ask_price_diff_6',\n",
    " 'ask_price_diff_7',\n",
    " 'ask_price_diff_8',\n",
    " 'ask_price_diff_9',\n",
    " 'ask_price_diff_10',\n",
    " 'bid_price_diff_1',\n",
    " 'bid_price_diff_4',\n",
    " 'bid_price_diff_5',\n",
    " 'bid_price_diff_6',\n",
    " 'bid_price_diff_7',\n",
    " 'bid_price_diff_8',\n",
    " 'bid_price_diff_9',\n",
    " 'bid_price_diff_10',\n",
    " 'ask_size_diff_1',\n",
    " 'ask_size_diff_2',\n",
    " 'ask_size_diff_3',\n",
    " 'ask_size_diff_4',\n",
    " 'ask_size_diff_5',\n",
    " 'ask_size_diff_6',\n",
    " 'ask_size_diff_7',\n",
    " 'ask_size_diff_8',\n",
    " 'ask_size_diff_9',\n",
    " 'ask_size_diff_10',\n",
    " 'bid_size_diff_1',\n",
    " 'bid_size_diff_2',\n",
    " 'bid_size_diff_3',\n",
    " 'bid_size_diff_4',\n",
    " 'bid_size_diff_5',\n",
    " 'bid_size_diff_6',\n",
    " 'bid_size_diff_7',\n",
    " 'bid_size_diff_8',\n",
    " 'bid_size_diff_9',\n",
    " 'bid_size_diff_10',\n",
    " 'weighted_wap_diff_1',\n",
    " 'weighted_wap_diff_2',\n",
    " 'weighted_wap_diff_3',\n",
    " 'weighted_wap_diff_4',\n",
    " 'weighted_wap_diff_5',\n",
    " 'weighted_wap_diff_6',\n",
    " 'weighted_wap_diff_7',\n",
    " 'weighted_wap_diff_8',\n",
    " 'weighted_wap_diff_9',\n",
    " 'price_spread_diff_1',\n",
    " 'price_spread_diff_2',\n",
    " 'price_spread_diff_3',\n",
    " 'price_spread_diff_4',\n",
    " 'price_spread_diff_5',\n",
    " 'price_spread_diff_6',\n",
    " 'price_spread_diff_7',\n",
    " 'price_spread_diff_8',\n",
    " 'price_spread_diff_9',\n",
    " 'price_spread_diff_10',\n",
    " 'price_change_diff_2',\n",
    " 'size_change_diff_2',\n",
    " 'price_change_diff_3',\n",
    " 'size_change_diff_3',\n",
    " 'price_change_diff_4',\n",
    " 'size_change_diff_4',\n",
    " 'price_change_diff_5',\n",
    " 'size_change_diff_5',\n",
    " 'price_change_diff_6',\n",
    " 'size_change_diff_6',\n",
    " 'price_change_diff_7',\n",
    " 'size_change_diff_7',\n",
    " 'price_change_diff_8',\n",
    " 'size_change_diff_8',\n",
    " 'price_change_diff_9',\n",
    " 'size_change_diff_9',\n",
    " 'price_change_diff_10',\n",
    " 'size_change_diff_10',\n",
    " 'rolling_diff_ask_price_2',\n",
    " 'rolling_std_diff_ask_price_2',\n",
    " 'rolling_std_diff_bid_price_2',\n",
    " 'rolling_diff_ask_size_2',\n",
    " 'rolling_std_diff_ask_size_2',\n",
    " 'rolling_diff_bid_size_2',\n",
    " 'rolling_std_diff_bid_size_2',\n",
    " 'rolling_diff_ask_price_3',\n",
    " 'rolling_std_diff_ask_price_3',\n",
    " 'rolling_diff_bid_price_3',\n",
    " 'rolling_std_diff_bid_price_3',\n",
    " 'rolling_diff_ask_size_3',\n",
    " 'rolling_std_diff_ask_size_3',\n",
    " 'rolling_diff_bid_size_3',\n",
    " 'rolling_std_diff_bid_size_3',\n",
    " 'rolling_diff_ask_price_4',\n",
    " 'rolling_std_diff_ask_price_4',\n",
    " 'rolling_diff_bid_price_4',\n",
    " 'rolling_diff_ask_size_4',\n",
    " 'rolling_std_diff_ask_size_4',\n",
    " 'rolling_diff_bid_size_4',\n",
    " 'rolling_std_diff_bid_size_4',\n",
    " 'rolling_diff_ask_price_5',\n",
    " 'rolling_std_diff_ask_price_5',\n",
    " 'rolling_diff_bid_price_5',\n",
    " 'rolling_std_diff_bid_price_5',\n",
    " 'rolling_diff_ask_size_5',\n",
    " 'rolling_std_diff_ask_size_5',\n",
    " 'rolling_diff_bid_size_5',\n",
    " 'rolling_std_diff_bid_size_5',\n",
    " 'rolling_diff_ask_price_6',\n",
    " 'rolling_std_diff_ask_price_6',\n",
    " 'rolling_diff_bid_price_6',\n",
    " 'rolling_std_diff_bid_price_6',\n",
    " 'rolling_diff_ask_size_6',\n",
    " 'rolling_std_diff_ask_size_6',\n",
    " 'rolling_diff_bid_size_6',\n",
    " 'rolling_std_diff_bid_size_6',\n",
    " 'rolling_diff_ask_price_7',\n",
    " 'rolling_std_diff_ask_price_7',\n",
    " 'rolling_diff_bid_price_7',\n",
    " 'rolling_std_diff_bid_price_7',\n",
    " 'rolling_diff_ask_size_7',\n",
    " 'rolling_std_diff_ask_size_7',\n",
    " 'rolling_diff_bid_size_7',\n",
    " 'rolling_std_diff_bid_size_7',\n",
    " 'rolling_diff_ask_price_8',\n",
    " 'rolling_std_diff_ask_price_8',\n",
    " 'rolling_diff_bid_price_8',\n",
    " 'rolling_std_diff_bid_price_8',\n",
    " 'rolling_diff_ask_size_8',\n",
    " 'rolling_std_diff_ask_size_8',\n",
    " 'rolling_diff_bid_size_8',\n",
    " 'rolling_std_diff_bid_size_8',\n",
    " 'rolling_diff_ask_price_9',\n",
    " 'rolling_std_diff_ask_price_9',\n",
    " 'rolling_diff_bid_price_9',\n",
    " 'rolling_std_diff_bid_price_9',\n",
    " 'rolling_diff_ask_size_9',\n",
    " 'rolling_std_diff_ask_size_9',\n",
    " 'rolling_diff_bid_size_9',\n",
    " 'rolling_std_diff_bid_size_9',\n",
    " 'rolling_diff_ask_price_10',\n",
    " 'rolling_std_diff_ask_price_10',\n",
    " 'rolling_diff_bid_price_10',\n",
    " 'rolling_diff_ask_size_10',\n",
    " 'rolling_std_diff_ask_size_10',\n",
    " 'rolling_diff_bid_size_10',\n",
    " 'rolling_std_diff_bid_size_10',\n",
    " 'mid_price*volume',\n",
    " 'harmonic_imbalance',\n",
    " 'seconds',\n",
    " 'minute',\n",
    " 'time_to_market_close',\n",
    " 'global_median_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecedb5cb",
   "metadata": {
    "papermill": {
     "duration": 0.008097,
     "end_time": "2023-12-19T14:00:56.720929",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.712832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a9a669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:00:56.738144Z",
     "iopub.status.busy": "2023-12-19T14:00:56.737783Z",
     "iopub.status.idle": "2023-12-19T14:07:20.581527Z",
     "shell.execute_reply": "2023-12-19T14:07:20.580516Z"
    },
    "papermill": {
     "duration": 383.855395,
     "end_time": "2023-12-19T14:07:20.584067",
     "exception": false,
     "start_time": "2023-12-19T14:00:56.728672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "10 qps: 2.3553168296813967\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "20 qps: 2.252022182941437\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "30 qps: 2.2329089721043904\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "40 qps: 2.2202364921569826\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "50 qps: 2.207497868537903\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "60 qps: 2.2040989001592\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "70 qps: 2.1977356331689015\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "80 qps: 2.1944733649492263\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "90 qps: 2.191134656800164\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "100 qps: 2.186951677799225\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "110 qps: 2.1853657093915073\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "120 qps: 2.184895279010137\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "130 qps: 2.1887176018494827\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "140 qps: 2.189597373349326\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "150 qps: 2.190354599952698\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "160 qps: 2.1913612321019174\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "Feat Shape is: (200, 305)\n",
      "The code will take approximately 2.5116 hours to reason about\n"
     ]
    }
   ],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices) / np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "    return out\n",
    "\n",
    "if is_infer:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 0\n",
    "    y_min, y_max = -64, 64\n",
    "    qps, predictions = [], []\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    # Weights for each fold model\n",
    "    if CAT:\n",
    "        models_lateuu=[]\n",
    "        for i in range(1,6):\n",
    "            model_filename = f\"/kaggle/input/datauu/modelitos_para_despues_cat_v3/doblez_{i}.model\"\n",
    "            m = joblib.load(model_filename)\n",
    "            models_lateuu.append(m)\n",
    "        model_filename = \"/kaggle/input/datauu/modelitos_para_despues_cat_v3/total_1.model\"\n",
    "        m = joblib.load(model_filename)\n",
    "        models_lateuu.append(m)\n",
    "        \n",
    "        models_datauu=[]\n",
    "        for i in range(1,6):\n",
    "            model_filename = f\"/kaggle/input/lateuu/modelitos_para_despues/doblez_{i}.model\"\n",
    "            m = joblib.load(model_filename)\n",
    "            models_datauu.append(m)\n",
    "        model_filename = \"/kaggle/input/lateuu/modelitos_para_despues/total_1.model\"\n",
    "        m = joblib.load(model_filename)\n",
    "        models_datauu.append(m)\n",
    "        \n",
    "        cat_model_weights = weighted_average(models_datauu)\n",
    "        #cat_model_weights=[0.3,0.3]\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        now_time = time.time()\n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "        feat = generate_all_features(cache)[-len(test):]\n",
    "        print(f\"Feat Shape is: {feat.shape}\")\n",
    "\n",
    "        # Generate predictions for each model and calculate the weighted average\n",
    "        if CAT:\n",
    "            cat_predictions = np.zeros(len(test))\n",
    "            for model, weight in zip(models_lateuu, cat_model_weights):\n",
    "                cat_predictions += weight * model.predict(feat[select_feature])\n",
    "            \n",
    "            lgb_predictions = np.zeros(len(test))\n",
    "            for model, weight in zip(models_datauu, cat_model_weights):\n",
    "                lgb_predictions += weight * model.predict(feat[feature_columns])\n",
    "                \n",
    "        predictions_cat = cat_predictions\n",
    "        predictions_lgb = lgb_predictions\n",
    "        \n",
    "        #Using mean predictions rather than zero sum\n",
    "        final_predictions_c = predictions_cat - np.mean(predictions_cat)\n",
    "        final_predictions_l = predictions_lgb - np.mean(predictions_lgb)\n",
    "        clipped_predictions_c = np.clip(final_predictions_c, y_min, y_max)\n",
    "        clipped_predictions_l = np.clip(final_predictions_l, y_min, y_max)\n",
    "        sample_prediction['target'] = (clipped_predictions_c+clipped_predictions_l)/2\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "        if counter % 10 == 0:\n",
    "            print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e2d8e",
   "metadata": {
    "papermill": {
     "duration": 0.023627,
     "end_time": "2023-12-19T14:07:20.631889",
     "exception": false,
     "start_time": "2023-12-19T14:07:20.608262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b25f0",
   "metadata": {
    "papermill": {
     "duration": 0.021581,
     "end_time": "2023-12-19T14:07:20.675398",
     "exception": false,
     "start_time": "2023-12-19T14:07:20.653817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482128a",
   "metadata": {
    "papermill": {
     "duration": 0.021702,
     "end_time": "2023-12-19T14:07:20.719646",
     "exception": false,
     "start_time": "2023-12-19T14:07:20.697944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4153709,
     "sourceId": 7197137,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4180898,
     "sourceId": 7222950,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4184026,
     "sourceId": 7236573,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4191721,
     "sourceId": 7237934,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4192052,
     "sourceId": 7238353,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 416.670369,
   "end_time": "2023-12-19T14:07:22.466462",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-19T14:00:25.796093",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
